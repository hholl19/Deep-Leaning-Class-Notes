{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "NLP_preprocessing_Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzjtZNkQVnzd"
      },
      "source": [
        "# Natural Language Preprocessing Using Keras Tokenizer Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ODxV_7Vnzf"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/the-deep-learners/deep-learning-illustrated/blob/master/notebooks/natural_language_preprocessing.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGUxgo_6Vnzg"
      },
      "source": [
        "#### Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cRjXFu2Vnzg"
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# import gensim\n",
        "from gensim.models.phrases import Phraser, Phrases\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmPTpy_EVnzh"
      },
      "source": [
        "#### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jGD9N5YIoIo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf-jC7akIXp2"
      },
      "source": [
        "df = pd.read_csv(\"gdrive/MyDrive/NLPData/imdb_data.csv\", sep=\",\", header=0)\n",
        "df.columns=['text']\n",
        "\n",
        "imbd_sents = list(df.text.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e13c2XvuSXR"
      },
      "source": [
        "word_tokens = [s.split() for s in imbd_sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5CVSCCYxGWu"
      },
      "source": [
        "len(word_tokens), len(imbd_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9k3HBCRumJ5"
      },
      "source": [
        "len(word_tokens[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXHxHu6uvQmb"
      },
      "source": [
        "stpwords = stopwords.words('english') + ['<br />']\n",
        "for i in range(len(word_tokens)):\n",
        "  word_tokens[i] = [w for w in word_tokens[i] if w.lower() not in stpwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE2aXRLq0v0o"
      },
      "source": [
        "' '.join(word_tokens[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-c1lNG-0G00"
      },
      "source": [
        "clean_sent = [' '.join(strlist) for strlist in word_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoOOxwdM1eSH"
      },
      "source": [
        "clean_sent[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUDHj7nZVnzh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "\n",
        "tokenizer.fit_on_texts(clean_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82S2NJIsljQg"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYOIHQN2Vnzi"
      },
      "source": [
        "encoded_tokens = tokenizer.texts_to_sequences(clean_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIl0TnSVmLkP"
      },
      "source": [
        "words = tokenizer.sequences_to_texts(encoded_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXZTxpyIpxVE"
      },
      "source": [
        "words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AViUP1RqqEOj"
      },
      "source": [
        "encoded_tokens[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi_cwYfH-QJO"
      },
      "source": [
        "#tokenizer.word_index\n",
        "index_word = {v:k for k,v in tokenizer.word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHghA5QKEC0a"
      },
      "source": [
        "' '.join(index_word[id] for id in encoded_tokens[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB90FmU1o9Uf"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded = pad_sequences(encoded_tokens, maxlen=100, padding='pre', truncating='pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywTd_NHNqWPK"
      },
      "source": [
        "# these sequences become the training data fed into your embedding layer\n",
        "padded[2]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}